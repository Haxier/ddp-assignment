summary(fit)$coef
fit <- lm(DriversKilled ~ pp + mmc + law, data = seatbelts)
summary(fit)$coef
fit <- lm(DriversKilled ~ pp + mmc + I(factor(law)), data = seatbelts)
summary(fit)$coef
seatbelts <- mutate(seatbelts, as.factor( (pp<= -1.5) + (pp<=0) + (pp <= 1.5) + (pp< inf)))
seatbelts <- mutate(seatbelts, as.factor( (pp<= -1.5) + (pp<=0) + (pp <= 1.5) + (pp< Inf)))
seatbelts <- mutate(seatbelts, ppf= as.factor( (pp<= -1.5) + (pp<=0) + (pp <= 1.5) + (pp< Inf)))
table(seatbelts$ppf)
fit <- lm(DriversKilled ~ ppf + mmc + law, data = seatbelts)
summary(fit)$coef
data("mtcars")
head(mtcars)
fit <- lm(mpg ~ I(factor(cyl) + wt))
fit <- lm(mpg ~ I(factor(cyl)) + wt, data = mtcars)
summary(fit)$coef
fit2 <- lm(mpg ~ I(factor(cyl)), data = mtcars)
summary(fit2)$coef
fit3 <- lm(mpg ~ I(factor(cyl)) * wt, data = mtcars)
summary(fit3)$coef
fit4 <- lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
summary(fit4)$coef
annova(fit2, fit3)
anova(fit2, fit3)
annova(fit1, fit3)
anova(fit1, fit3)
anova(fit, fit3)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit_hat <- lm(y ~ x)
hatvalues(fit_hat)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit_hat <- lm(y ~ x)
dfbetas(fit_hat)
library(swirl)
swirl()
rgp1()
rgp2
rgp2()
head(swiss)
mdl <- lm(Fertility ~ . , data = swiss)
vif(mdl)
mdl2 <- lm(Fertility ~ . - Examination, data = swiss)
vif(mdl2)
library()
library(swirl)
swirl()
simbias()
x1c <- simbias()
aaply(x1c, 1, mean)
apply(x1c, 1, mean)
fit1 <- lm(Fertility ~ Agriculture, data = swiss)
fit3 <- lm(Fertility ~ Agriculture + Examination + Education, data = swiss)
anova(fit1, fit3)
deviance(fit3)
d <- deviance(fit3)/43
n <- (deviance(fit1)-deviance(fit3))/2
n/d
pf(n/d, 2, 43, lower.tail = FALSE)
shapiro.test(fit3$residuals)
anova(fit1,fit3, fit5, fit6)
View(ravenData)
mdl <- glm(ravenWinNum ~ ravenScore, family = "binomial", data = ravenData)
lodds <- predict(mdl, data.frame(ravenScore= c(0, 3, 6)))
exp(lodds)/ (1+exp(lodds))
summary(mdl)
exp(confint(mdl))
anova(mdl)
qchisq(.95,1)
var(rpois(1000, 50))
nxt()
head(hits)
class(hist[ ,1])
class(hist[ ,'date'])
class(hits[ ,1])
as.integer(head(hits[,1]))
mdl <- glm(visits ~ date, family= "poisson", data=hits)
summary(mdl)
exp(confint(mdl, "date"))
which.max(hits[,"visits"])
hits[704,]
lambda <- mdl$fitted.values[704]
qpois(.95, lambda)
1
mdl2 <-  glm(simplystats ~ date, family="poisson", data = hits, offset = log(visits+1))
qpois(.95, mdl2$fitted.values[704])
library(datasets)
data("Seatbelts")
belts <- as.data.frame(Seatbelts)
head(belts)
mdl <- lm(DriversKilled ~ kms + PetrolPrice + law, data = belts)
resid(mdl)
par(mfrow= c(2,2))
plot(mdl)
summary(mdl)
summary(mdl)$sigma^2
dfbetas(mdl)
dffits(mdl)
summary(dffits(mdl))
summary(dffits(mdl))
summary(hatvalues(mdl))
library(datasets)
data("Seatbelts")
belts <- as.data.frame(Seatbelts)
library(dplyr)
belts <- mutate(belts, pp= (PetrolPrice - mean(PetrolPrice))/sd(PetrolPrice), mm=kms/1000, mmc=mm - mean(mm))
fit <- glm(DriversKilled ~ pp + mmc + law, family = poisson, data = belts)
round(summary(fit)$coef, 3)
1 - exp(-.115)
fit2 <-  lm(I(log(DriversKilled)) ~ pp + mmc + law, data = belts)
round(summary(fit2)$coef, 3)
fit3 <- glm(DriversKilled ~ pp + mmc + law, family = poisson, offset = log(drivers+1), data = belts)
summary(fit3)$coef
round(summary(fit3)$coef,3)
fit3 <- glm(DriversKilled ~ pp + mmc + law, family = poisson, offset = log(drivers), data = belts)
summary(fit3)$coef
round(summary(fit3)$coef,3)
fit0 <- glm(DriversKilled ~ law, family = poisson, data = belts)
fit1 <- glm(DriversKilled ~ pp + law, family = poisson, data = belts)
fit2 <- glm(DriversKilled ~ pp + mmc +law, family = poisson, data = belts)
anove(fit0, fit1, fit2)
anova(fit0, fit1, fit2)
library(MASS)
data("shuttle")
?shuttle
head(shuttle)
fit <- glm(use ~ wind, family = binomial, data = shuttle)
summary(fit)$coef
library(dplyr)
shut <- mutate(shuttle, usebin= 1*(use== "auto"))
head(shut)
shut <- mutate(shuttle, windbin= 1*(wind== "head"))
fit <- glm(usebin ~ windbin, family = binomial, data = shut)
shut <- mutate(shuttle, windbin= 1*(wind== "head"), usebin= 1*(use=="auto"))
fit <- glm(usebin ~ windbin, family = binomial, data = shut)
coef(fit)
exp(coef(fit))
summary(fit)$coef
fit1 <- glm(usebin ~ windbin + magn, family = binomial, data = shut)
summary(fit1)$coef
exp(-3.200873e-02)
library(datasets)
data("InsectSprays")
head(InsectSprays)
fit4 <- glm(count ~ spray, family = poisson, data = InsectSprays)
summary(fit4)$coef
exp(0.05588046)
relevel(InsectSprays$spray, "B")
spraydata <- relevel(InsectSprays$spray, "B")
fit4 <- glm(count ~ spraydata, family = poisson, data = InsectSprays)
summary(fit4)$coef
exp(-0.05588046)
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
plot(x, y, frame=FALSE, pch=21, bg="lightblue", cex=2)
z <- (x > 0) * x
fit <- lm(y ~ x + z)
sum(coef(fit)[2:3]) #1.013
Status API Training Shop Blog About
lines(fit)
abline(fit)
library(datasets)
data(mtcars)
summary(mtcars)
?mtcars
pairs(mtcars)
mdl <- lm(mpg ~ am, data = mtcars)
mummary(mdl)
summary(mdl)
summary(mdl)$coef
str(mtcars)
mdl <- lm(mpg ~ am, data = mtcars)
summary(mdl)
par(mfrow= c(2,2))
plot(mdl)
pairs(mtcars)
plot(mdl)
mdl2 <- lm(mpg ~ ., data = mtcars)
summary(mdl2)$coef
anova(mdl, mdl2)
plot(mdl2)
mdl2 <- lm(mpg ~ ., data = mtcars)
summary(mdl2)$coef
install.packages("bestglm")
?bestmodel
library(bestglm)
out <- bestglm(mtcars)
names(out)
head(out)
out$BestModels[am==TRUE]
out$BestModels[, am==TRUE]
class(out$BestModels)
models <- out$BestModels
models
summary(mdl2)$coef
class(summary(mdl2)$coef)
wt + am + qsec + hp
test_model <- lm(mpg ~ wt + hp + qsec + am, data = mtcars)
summary(test_model)$coef
finalmodel <- lm(mpg ~ wt + qsec + am, data = mtcars)
summary(finalmodel)$coef
par(mfrow=c(2,2))
plot(finalmodel)
plot(test_model)
anova(mdl, test_model, finalmodel, mdl2)
anova(mdl, finalmodel, test_model, mdl2)
summary(finalmodel)
install.packages(c("BH", "cluster", "curl", "DBI", "devtools", "dplyr", "evaluate", "fields", "formatR", "git2r", "googlesheets", "highr", "Hmisc", "httr", "jsonlite", "knitr", "lazyeval", "lubridate", "Matrix", "nlme", "openssl", "plotrix", "plyr", "purrr", "quantreg", "Rcpp", "rmarkdown", "rsconnect", "rstudioapi", "rvest", "stringi", "survival", "swirl", "testthat", "tidyr", "withr", "xml2", "zoo"))
install.packages(c("BH", "cluster", "curl", "DBI", "devtools",
))
install.packages(c("BH", "cluster", "curl", "DBI", "devtools", "dplyr", "evaluate", "fields", "formatR", "git2r", "googlesheets", "highr", "Hmisc", "httr", "jsonlite", "knitr", "lazyeval", "lubridate", "Matrix", "nlme", "openssl", "plotrix", "plyr", "purrr", "quantreg", "Rcpp", "rmarkdown", "rsconnect", "rstudioapi", "rvest", "stringi", "survival", "swirl", "testthat", "tidyr", "withr", "xml2", "zoo"))
install.packages(c("BH", "cluster", "curl", "DBI", "devtools",
))
install.packages("survival")
library(swirl)
install.packages("swirl")
install.packages("ggplot2")
install.packages("caret")
install.packages("kernlab")
library(caret)
library(kernlab)
data(spam)
inTrain <- createDataPartition(spam$type, p= .75, list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
dim(training)
set.seed(32343)
modelFit <- train(type ~ . , data = training, method = "gml")
modelFit <- train(type ~ . , data = training, method = "glm")
install.packages("e1071")
modelFit <- train(type ~ . , data = training, method = "glm")
modelFit
modelFit$finalModel
predictions <- predict(modelFit, data= testing)
predictions
confusionMatrix(predictions, testing$type)
predictions <- predict(modelFit, data= testing)
confusionMatrix(predictions, testing$type)
dim(predictions)
lenght(predictions)
length(predictions)
length(testing$type)
predictions <- predict(modelFit, newdata= testing)
length(predictions)
confusionMatrix(predictions, testing$type)
set.seed(32323)
folds <- createFolds(y= spam$type, k=10, list = TRUE, returnTrain = TRUE)
sapply
sapply(folds, length)
install.packages("ISLR")
library(ISLR)
library(ggplot2)
data("Wage")
summary(Wage)
inTrain <- createDataPartition(Wage$wage, p=.7, list = FALSE)
training <- Wage[inTrain, ]
testing <- Wage[-inTrain, ]
dim(training)
dim(testing)
featurePlot(x= training[ , c("age", "education", "jobclass")], y=training$wage, plot = "pairs")
qplot(age, wage, data = training)
qplot(age, wage, colour="jobclass", data = training)
qplot(age, wage, colour=jobclass, data = training)
qq <- qplot(age, wage, colour=education, data = training)
qq + geom_smooth(method = "lm", formula = y ~ x)
install.packages("Hmisc")
qplot(wage, colour= education, data = training, geom = "density")
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
names(AlzheimerDisease)
head(predictors)
head(diagnosis)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
head(training)
qplot(CompressiveStrength, data = training)
suppressMessages(library(dplyr))
suppressMessages(library(Hmisc))
suppressMessages(library(gridExtra))
training <- mutate(training, index=1:nrow(training))
cutIndex <- cut2(training$index, g=10)
breaks <- 10
qplot(index, CompressiveStrength, data=training, color=cut2(training$Cement, g=breaks))
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(training$Superplasticizer)
hist(log(training$Superplasticizer+1))
par(mfrow=c(1,2))
hist(training$Superplasticizer)
hist(log(training$Superplasticizer+1))
hist(log(training$Superplasticizer))
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433); data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433); data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]; training = adData[ inTrain,]
testing = adData[-inTrain,]
names(training)
smallIL <- training[ ,58:70]
names(smallIL)
smallIL <- training[ ,58:69]
names(smallIL)
preObj <- preProcess(smallIL, method = "pca", thresh = .9)
preObj
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433); data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]; training = adData[ inTrain,]
testing = adData[-inTrain,]
smallTraining <- training[, 50:69]
smallTraining$diagnosis <- training$diagnosis
names(smallTraining)
smallTraining <- training[, 58:69]
smallTraining$diagnosis <- training$diagnosis
names(smallTraining)
model1 <- train(smallTraining$diagnosis, method = "glm")
model1 <- train(y=smallTraining$diagnosis, method = "glm")
model1 <- train(diagnosis ~., data = smallTraining, method = "glm")
model2 <- train(diagnosis ~., data = smallTraining, method = "glm", preProcess= "pca")
results1 <- predict(model1, testing)
results2 <- predict(model2, testing)
confusionMatrix(results1)
confusionMatrix(results1, testing$diagnosis)
confusionMatrix(results2, testing$diagnosis)
install.packages("rattle")
library(caret)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
names(segmentationOriginal)
dim(segmentationOriginal)
inTrain <- createDataPartition(y=segmentationOriginal$Case, p=.7, list = FALSE)
training <- segmentationOriginal[inTrain, ]
testing  <- segmentationOriginal[-inTrain, ]
set.seed(125)
modFit <- train(Case ~. , data = segmentationOriginal, method= "rpart")
print(modFit$finalModel)
predict(modFit, newdata = data.frame(TotalIntench2 = 23000, FiberWidthCh1 = 10, PerimStatusCh1=2 ))
data.frame(TotalIntench2 = 23000, FiberWidthCh1 = 10, PerimStatusCh1=2 )
testing <- data.frame(TotalIntench2 = 23000, FiberWidthCh1 = 10, PerimStatusCh1=2 ))
testing <- data.frame(TotalIntench2 = 23000, FiberWidthCh1 = 10, PerimStatusCh1=2 )
predict(modFit, newdata = testing
)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain <- createDataPartition(y=segmentationOriginal$Case, p=.7, list = FALSE)
training <- segmentationOriginal[inTrain, ]
testing  <- segmentationOriginal[-inTrain, ]
set.seed(125)
modFit <- train(Case ~. , data = segmentationOriginal, method= "rpart")
testing <- data.frame(TotalIntench2 = 23000, FiberWidthCh1 = 10, PerimStatusCh1=2 )
predict(modFit, newdata = testing)
testing <- data.frame(TotalIntench2 = 50000, FiberWidthCh1 = 10, VarIntenCh4 = 100)
predict(modFit, newdata = testing)
head(segmentationOriginal$TotalIntench2)
head(segmentationOriginal$TotalIntench2)
head(segmentationOriginal$TotalIntenCh2)
head(segmentationOriginal$FiberWidthCh1)
head(segmentationOriginal$FiberWidthCh1)
head(segmentationOriginal$VarIntenCh4)
testing <- data.frame(TotalIntenCh2 = 50000, FiberWidthCh1 = 10, VarIntenCh4 = 100)
predict(modFit, newdata = testing)
head(segmentationOriginal$Case)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
head(segmentationOriginal$Case)
summary(segmentationOriginal$Case)
head(segmentationOriginal$Case)
View(testing)
View(cars)
View(segmentationOriginal)
head(segmentationOriginal$Case)
rm(segmentationOriginal)
rm(cars, dat, datuak, father, galton, mtcars)
rm(cars, dat, datuak, father.son, galton, mtcars)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
head(segmentationOriginal$Case)
# 1. Subset the data to a training set and testing set based on the Case variable in the data set.
inTrain <- createDataPartition(y = segmentationOriginal$Case, p = 0.6,
list = FALSE) # 60% training
training <- segmentationOriginal[inTrain, ]
testing <- segmentationOriginal[-inTrain, ]
# 2. Set the seed to 125 and fit a CART model with the rpart method using all predictor variables and default caret settings. (The outcome class is contained in a factor variable called Class with levels "PS" for poorly segmented and "WS" for well segmented.)
set.seed(125)
modFit <- train(Class ~ ., method = "rpart", data = training)
suppressMessages(library(rattle))
librry(rattle())
library(rattle())
library(rattle
)
fancyRpartPlot(modFit$finalModel)
install.packages(rattle)
install.packages("rattle")
install.packages("rattle")
library(rattle)
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages("rpart.plot")
library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
names(olive)
dim(olive)
modFit <- train(Area ~., data=olive, method= "rpart")
library(caret)
modFit <- train(Area ~., data=olive, method= "rpart")
fancyRpartPlot(modFit$finalModel)
head(olive)
newdata = as.data.frame(t(colMeans(olive)))
testing <- newdata = as.data.frame(t(colMeans(olive)))
testing <- data.frame(t(colMeans(olive)))
head(testing)
pred <- predict(modFit, newdata= testing)
summary(pred)
pred
class(olive$Area)
summary(olive$Area)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
set.seed(13234)
names(SAheart)
modFit <- train(chd ~ age+alcohol+obesity+tobacco+typea+ldl, data=trainSA, method="glm")
modFit <- train(chd ~ age+alcohol+obesity+tobacco+typea+ldl, data=trainSA, method="glm", family="binomial")
set.seed(13234)
modelSA <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
data = trainSA, method = "glm", family = "binomial")
missClass(testSA$chd, predict(modelSA, newdata = testSA))
missClass(trainSA$chd, predict(modelSA, newdata = trainSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modFit <- train(y ~ ., data=vowel.train, method="rf")
?varImp
varImp(modFit, useModel = "rf")
varImp(modFit)
order(varImp(modvowel), decreasing = T)
order(varImp(modFit), decreasing = T)
modvowel <- randomForest(y ~ ., data = vowel.train)
order(varImp(modvowel), decreasing = T)
varImp(modvowel)
library(rCharts)
hairere <- as.data.frame(HairEyeColor)
n <- nPlot(Freq ~ Hair, group="Eye", type= "multiBarChart", data= subset(hairere, Sex=="Male"))
n
names(iris) = gsub("\\.", "", names(iris))
rPlot(SepalLength ~ SepalWidth | Species, data = iris, color = 'Species', type = 'point')
hair_eye = as.data.frame(HairEyeColor)
rPlot(Freq ~ Hair | Eye, color = 'Eye', data = hair_eye, type = 'bar')
library(googleVis)
M <- gvisMotionChart(Fruits, "Fruit", "Year", options= list(width=600, height=400))
plot(M)
predict
colSums
show
dgamma
setwd("~/Documents/DataScience/ddp/assignment")
library(datasets)
data("cars")
dim(cars)
shiny::runApp()
runApp()
runApp()
runApp()
